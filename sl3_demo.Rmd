---
title: "A short and sweet demo of `sl3`"
subtitle: "R-Ladies NYC Meetup"
author: "Kat Hoffman"
date: "September 10, 2019"
output:
  html_document:
    theme: "cosmo"
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Set up

## Load Libraries

```{r}
#devtools::install_github("tlverse/sl3")
library(sl3)
library(dplyr)
library(kableExtra)
```

## Set a seed

For reproducible results

```{r}
set.seed(7)
```

# Data Prep

## Load the data

Using the WASH Benefits data set we will predict children in rural Kenya and Bengladesh's weight to height z-scores from their housing and sanitation states.

```{r}
washb_data <- read.csv("https://raw.githubusercontent.com/tlverse/tlverse-data/master/wash-benefits/washb_data.csv")
```

## View the data

```{r}
washb_data %>% head(n = 20)
```

## Specify outcome and predictors

We need the columns specified as strings.

```{r}
outcome <- "whz"
covars <- washb_data %>%
  select(-whz) %>%
  names()
```

## Make an sl3 task

This is the object we'll fit our model on.

```{r}
washb_task <- make_sl3_Task(
  data = washb_data,
  covariates = covars,
  outcome = outcome
)
```

We can't have missing data, so `sl3`'s default pre-processing imputes at the median and adds a column for missingness (in case the missingness is informative).

```{r}
washb_task
```

# Explore `sl3`'s options

There's a ton of different aspects of model fitting `sl3` has the capabilities to address!

```{r}
sl3_list_properties()
```

## Look at available "learners"

We'll need to pick out base learners for our stack, as well as pick a metalearner. Since we are trying to predict z-scores, a continuous variable, let's look at our potential learners for a continuous variable.

```{r}
sl3_list_learners("continuous") 
```

You'll notice each learner starts with `Lrnr` and seems to correspond to a package in `R`. You can learn about each learner here: https://tlverse.org/sl3/reference/index.html

# Superlearner setup!

## Choose base learners

Let's pick just a few base learners to match the examples in my slides: a random forest, a generalized boosting model, and a generalized linear model. Let's keep their default parameters for now.

`make_learner_stack()` is an easy way to create a stack of default baselearners. It takes the names of the learners as strings and you're good to go!

```{r}
stack <- make_learner_stack(
  "Lrnr_randomForest", 
  "Lrnr_gbm",
  "Lrnr_glm"
)
```

## Choose a metalearner

There are many models we can choose from but we'll keep it simple and use a generalized linear model. We are again using the `make_learner()` function.

```{r}
metalearner <- make_learner(Lrnr_glm)
```

## Make a superlearner object

Remember, under-the-hood `Lrnr_sl` takes the cross-validated predictions from the base models and uses them to predict the true outcome. That prediction model then is used to fit the predictions from base learners fit on the whole data set.

```{r}
sl <- make_learner(Lrnr_sl, 
                   learners = stack,
                   metalearner = metalearner)
```

A superlearner object has different functions built into it, such as `train()`.
We can train our superlearner shell model on the task we made earlier.

# TRAIN your Superlearner

```{r}
sl_fit <- sl$train(washb_task)
```

# Examine the results

## Examine coefficients and CV-risk

The default risk is MSE (Mean Squared Error). The coefficients show you how the metalearner decided to weight each base model for the final ensemble.

```{r}
sl_fit$print() %>% kable() %>% kable_styling(c("striped","condensed","hover"))
```

## Look at the predictions

`predict()` allows you to see what the model predicts on any given task. Here we look at predictions from the same data we trained the superlearner on, so the predicted weight to height z-scores of the first six children in our data set.

```{r}
sl_fit$predict(washb_task) %>% head()
```

# What else?!

- Use `make_learner()` to customize the parameters of your base learners or metalearner. Ex: `lrnr_RF_200trees <- make_learner(Lrnr_randomForest, ntree = 200)`

- Cross validate your entire ensembled superlearner using the cross-validation package `origami`, written by the same authors as `sl3`. Or just hold out a testing data set to evaluate performance.

- Add many layers to your superlearner and organize it into a "pipeline"

**For more demos, check out the following teaching materials from the authors of `sl3`.** My tutorial uses one of their example data sets in case you'd like to extend your learning via their training resources.

- https://tlverse.org/tlverse-handbook/ensemble-machine-learning.html

- https://tlverse.org/acic2019-workshop/ensemble-machine-learning.html

- https://github.com/tlverse/sl3_lecture
